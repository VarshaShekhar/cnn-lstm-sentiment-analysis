
# ğŸ¬ CNN-LSTM Sentiment Analysis on IMDB Movie Reviews

ğŸš€ A hands-on **Natural Language Processing (NLP)** project that implements and compares **LSTM** and **CNN + LSTM** deep learning models for **sentiment classification** of movie reviews.

This project demonstrates practical applications of **Deep Learning for Text**, including **text preprocessing, sequence modeling, hybrid architectures, model evaluation, and result comparison**.

---

![Project Thumbnail](cnn_lstm.png)

---
## ğŸ“Œ Overview

* **Goal**: Classify IMDB movie reviews as **positive** or **negative**
* **Focus Areas**:

  * Text preprocessing and sequence padding
  * Word embeddings for textual representation
  * LSTM-based sentiment classification
  * Hybrid CNN + LSTM architecture
  * Model evaluation using Accuracy and F1-score
  * Comparative analysis of models
* **Framework Used**: TensorFlow + Keras
* **Evaluation Metrics**: Accuracy, F1 Score

---

## ğŸ§  Problem Statement

Movie reviews are written in natural language and vary in length and structure.
The task is to **automatically determine the sentiment** (positive or negative) of a given review.

This problem is a classic **binary text classification task**, widely used to evaluate NLP models and deep learning architectures.

---

## ğŸ“Š Dataset

* **IMDB Movie Review Dataset**
* 25,000 training samples and 25,000 test samples
* Binary sentiment labels:

  * `0` â†’ Negative
  * `1` â†’ Positive
* Dataset provided directly by **Keras**

---

## ğŸ› ï¸ Technologies Used

* Python ğŸ
* NumPy ğŸ“Š
* TensorFlow & Keras ğŸ§ 
* scikit-learn âš™ï¸
* Jupyter Notebook ğŸ““

---

## âœ¨ Key Features

* ğŸ“ **Text Preprocessing**: Tokenized and padded review sequences
* ğŸ”¢ **Word Embeddings**: Dense vector representation of words
* ğŸ” **LSTM Model**: Captures sequential and contextual dependencies in text
* ğŸ§© **CNN + LSTM Model**:

  * CNN extracts local n-gram features
  * LSTM captures long-term dependencies
* ğŸ§® **Evaluation Metrics**: Accuracy and F1 score
* ğŸ” **Prediction Analysis**: Displays predictions for random test samples
* ğŸ“ **Architecture Diagrams**: Visual explanation of model structures

---

## ğŸ§© Project Structure

### ğŸ”¹ LSTM Model

* **Purpose**: Learn sentiment from sequential word dependencies
* **Architecture**:

```
Input â†’ Embedding â†’ LSTM â†’ Dense (Sigmoid)
```

* Serves as the **baseline model** for comparison.

---

### ğŸ”¹ CNN + LSTM Model

* **Purpose**: Combine local feature extraction with sequence modeling
* **Architecture**:

```
Input â†’ Embedding â†’ Conv1D â†’ MaxPooling â†’ LSTM â†’ Dense (Sigmoid)
```

* CNN captures local word patterns (n-grams)
* LSTM captures contextual and temporal information

---

### ğŸ”¹ Training & Evaluation

* Models trained on IMDB training data
* Evaluated on unseen test data
* Metrics used:

  * **Accuracy**
  * **F1 Score**
* Same test samples are used to compare predictions across both models

---

## ğŸ“ˆ Results Summary

| Model      | Accuracy | F1 Score |
| ---------- | -------- | -------- |
| LSTM       | High     | High     |
| CNN + LSTM | High     | High     |

> *Exact values may vary slightly depending on training configuration and random initialization.*

---

## ğŸ“ Files

* `CNN_LSTM_Text_Classification_IMDB.ipynb` â€“ Complete notebook with:

  * Data preprocessing
  * Model implementation
  * Training and evaluation
  * Predictions and comparison
* `images/` â€“ Architecture diagrams for LSTM and CNN + LSTM models

---

## ğŸ§¾ Notes

* Training time may vary depending on CPU/GPU availability
* Some FutureWarnings from Keras/NumPy may appear and can be safely ignored
* This project is intended for **learning and demonstration purposes**

---

## ğŸ¤ Connect

- [LinkedIn](https://www.linkedin.com/in/varsha-shekhar)
- [Gmail](varshaiyer96@gmail.com)
